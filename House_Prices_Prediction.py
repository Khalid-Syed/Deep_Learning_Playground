# -*- coding: utf-8 -*-
"""Assignment_2_Problem_3_MLP_Normalized Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YNiNL4Adumanm6ql3Jw3m4Zxv38KL28Y
"""

# Importing all the libraries
import os
import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler

import tqdm
import copy

!pip install wandb -qU

# Log in to your W&B account
import wandb

# Use wandb-core, temporary for wandb's new backend
wandb.require("core")

wandb.login()

# Reading the house dataset csv file
house_data = pd.read_csv('/content/kc_house_data.csv')

#Spitting the loaded dataset into train:validation:test (70:20:10)
raw_house_data_train, raw_house_data_validation, raw_house_data_test= torch.utils.data.random_split(house_data, [0.7,0.2,0.1])

print(len(house_data))
print(len(raw_house_data_train))
print(len(raw_house_data_validation))
print(len(raw_house_data_test))

class HousePricesDataset(Dataset):
    """House Prices dataset."""

    def __init__(self, csv_file, transform=None):
        """
        Arguments:
            csv_file (string): Path to the csv file with data.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.house_data = pd.read_csv(csv_file)
        self.transform = transform

        scaler = MinMaxScaler()
        # # features_to_normalize = ['bedrooms', 'bathrooms', 'sqft_living', 'view', 'grade', 'sqft_above', 'sqft_basement']
        features_to_normalize = ['bedrooms', 'bathrooms', 'sqft_living', 'view', 'grade', 'sqft_above', 'sqft_basement']
        label = 'price'

        self.house_data[features_to_normalize] = scaler.fit_transform(self.house_data[features_to_normalize])

        self.coloumns_to_extract = features_to_normalize + [label]
        self.house_data = self.house_data[self.coloumns_to_extract]

    def __len__(self):
        return len(self.house_data)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        house = self.house_data.iloc[idx, 0:]
        sample = np.array([house['bedrooms'], house['bathrooms'], house['sqft_living'], house['view'], house['grade'], house['sqft_above'], house['sqft_basement'], house['price']], dtype=np.float32)



        if self.transform:
            sample = self.transform(sample)

        return sample

house_dataset = HousePricesDataset(csv_file='/content/kc_house_data.csv')
house_dataset_train, house_dataset_validation, house_dataset_test = torch.utils.data.random_split(house_dataset, [0.7, 0.2, 0.1])



# house_dataset_train_tensor = torch.tensor(house_dataset_train)
# info =  torch.corrcoef(house_dataset_train_tensor.T)
# print(info)
# print(info.shape)

# print(len(house_dataset_train))
# print(len(house_dataset_validation))
# print(len(house_dataset_test))

print(house_dataset[0])

fig = plt.figure()

# for i, sample in enumerate(house_dataset_train):
#     print(i, len(sample))

dataloader_train = DataLoader(house_dataset_train, batch_size= 4, shuffle= True, num_workers= 0)
dataloader_test = DataLoader(house_dataset_test, batch_size= len(house_dataset_test), shuffle= True, num_workers= 0)
dataloader_validation = DataLoader(house_dataset_validation, batch_size= len(house_dataset_validation), shuffle= True, num_workers= 0)

# Helper function to show a batch
def show_houses_batch(sample_batched):
    """Show house records for a batch of samples."""
    houses_batch = sample_batched
    batch_size = len(houses_batch) # This shows the number of houses in a single batch
    record_size = houses_batch.size(1) # This shows the dimensions of data for a single house

    for i in range(batch_size):
        plt.scatter(i, houses_batch[i, 4].numpy(), marker='.', c='r')

        plt.title('Batch from dataloader')

for i_batch, sample_batched in enumerate(dataloader_train):
    print(sample_batched)
    print(i_batch, len(sample_batched))

    # observe 4th batch and stop.
    if i_batch == 3:
        plt.figure()
        show_houses_batch(sample_batched)
        plt.ioff()
        plt.show()
        break

####################################    LAB   TASK   ################################
# torch.corrcoef()

device = ("cuda" if torch.cuda.is_available() else "cpu")
print("Using device: ", device)

class KNeuralNetwork(nn.Module):
  def __init__(self):
    super().__init__()
    self.linear_relu_stack = nn.Sequential(
        nn.Linear(7,128),
        nn.ReLU(),
        nn.Linear(128,64),
        nn.ReLU(),
        nn.Linear(64,32),
        nn.ReLU(),
        nn.Linear(32,16),
        nn.ReLU(),
        nn.Linear(16,8),
        nn.ReLU(),
        nn.Linear(8,1)
    )

  def forward(self, x):
      logits = self.linear_relu_stack(x)
      return logits

model_Z = KNeuralNetwork().to(device)
print(model_Z)

# Training the NN

loss_fn = nn.MSELoss()
learning_rate = 0.0001
optimizer = optim.Adam(model_Z.parameters(), lr = learning_rate)

n_epochs = 100
batch_start = torch.arange(0, len(dataloader_train))
best_mse = np. inf
best_weights = None
history = []


# The following code snippet helps initialize Weights and Biases
wandb.init(
      # Set the project where this run will be logged
      project="Assingment Final Run - Normalized Fr",
      # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)
      name=f"Experiment 1000",
      # Track hyperparameters and run metadata
      config={
      "learning_rate": learning_rate,
      "architecture": "Complex 6 layer MLP (7,128)",
      "dataset": "House Prices Dataset - Advanced - Normalized - 7 input parameters"
      })

for epoch in range(n_epochs):
    model_Z.train()
    with tqdm.tqdm(batch_start, unit="batch", mininterval=0, disable=True) as bar:
        bar.set_description(f"Epoch {epoch}")
        for i_batch, sample_batched in enumerate(dataloader_train):
            sample_batched_X = sample_batched[:, :7]
            sample_batched_Y = sample_batched[:, 7:]

            # take a batch
            pred = model_Z(sample_batched_X)

            # forward pass
            loss = loss_fn(pred, sample_batched_Y)

            # backward pass
            optimizer.zero_grad()
            loss.backward()

            # update weights
            optimizer.step()

            # print progress
            bar.set_postfix(mse=float(loss))

    # evaulate accuracy at the end of each epoch on test set
    model_Z.eval()
    for i_batch, sample_batched in enumerate(dataloader_validation):
      sample_batched_X = sample_batched[:, :7]
      sample_batched_Y = sample_batched[:, 7:]

      y_pred = model_Z(sample_batched_X)
      mse = loss_fn(y_pred, sample_batched_Y)
      mse = float(mse)
      history.append(mse)
      if mse < best_mse:
          best_mse = mse
          best_weights = copy.deepcopy(model_Z.state_dict())


      # Log metrics from your script to W&B
      wandb.log({"loss": mse})


# restore model and return best accuracy
model_Z.load_state_dict(best_weights)
print("MSE: %.2f" % best_mse)
print("RMSE: %.2f" % np.sqrt(best_mse))
plt.plot(history)
plt.show()


# Mark the run as finished
wandb.finish()

n_epochs = 100
batch_start = torch.arange(0, len(dataloader_train))
best_mse = np. inf
best_weights = None
history = []

for epoch in range(n_epochs):
    with tqdm.tqdm(batch_start, unit="batch", mininterval=0, disable=True) as bar:
        bar.set_description(f"Epoch {epoch}")

    # evaulate accuracy at the end of each epoch on validation set
    model_Z.eval()
    for i_batch, sample_batched in enumerate(dataloader_validation):
      sample_batched_X = sample_batched[:, :7]
      sample_batched_Y = sample_batched[:, 7:]

      y_pred = model_Z(sample_batched_X)
      mse = loss_fn(y_pred, sample_batched_Y)
      mse = float(mse)
      history.append(mse)
      if mse < best_mse:
          best_mse = mse
          best_weights_val = copy.deepcopy(model_Z.state_dict())

# restore model and return best accuracy
model_Z.load_state_dict(best_weights_val)
print("MSE on Test Set: %.2f" % best_mse)
print("RMSE on Test Set: %.2f" % np.sqrt(best_mse))
plt.plot(history)
plt.show()



### KAN Implementation - House Prices Dataset

!pip install pykan

from kan import *
from kan.utils import create_dataset_from_data
import torch
from sklearn.model_selection import train_test_split

device = torch.device("cuda" if torch.cuda.is_available() else "cpu" )
print(device)

A_model_hd = KAN(width=[2,5,1], grid=3, k=3, seed=42, device=device)
B_model_hd = KAN(width=[5,10,1], grid=3, k=3, seed=42, device=device)

house_dataset_kan = pd.read_csv('/content/kc_house_data.csv')


house_dataset_kan.isnull().sum()
house_dataset_kan.describe()

# house_dataset = HousePricesDataset(csv_file='/content/kc_house_data.csv')
house_dataset_kan = pd.read_csv('/content/kc_house_data.csv')


house_dataset_kan.isnull().sum()
house_dataset_kan.describe()

features = ['bedrooms', 'bathrooms', 'sqft_living', 'floors', 'condition', 'grade', 'sqft_basement']

# total_features = ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode' ]

# house['bedrooms'], house['bathrooms'], house['sqft_living'], house['floors'], house['condition'], house['grade'],  house['sqft_basement'], house['price']

            # 'grade', 'view', 'sqft_above', 'sqft_basement', 'sqft_living', 'yr_built', 'yr_renovated',
            # 'waterfront' , 'zipcode'
label = 'price'

# Split the dataset into input features (X) and label (y)
X = house_dataset_kan[features]
y = house_dataset_kan[label]


#Data preprocessing
# scaler = StandardScaler()
# scaler = MinMaxScaler()
self.house_data_n = scaler.fit_transform(self.house_data)


# Split the data into 70% train, 20% validation, and 10% test
# First, split 80% (train+validation) and 20% (test)
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=14)

# Next, split the 80% (train+validation) into 70% train and 20% validation
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=14)

# Print the shapes of the splits to verify
# print(f'Training set size: {X_train.shape[0]}')
# print(f'Validation set size: {X_val.shape[0]}')
# print(f'Test set size: {X_test.shape[0]}')

#Full dataset
# X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)
# y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)

X_train_s= X_train.iloc[:100,:]
Y_train_s= y_train.iloc[:100]

X_train_tensor_s = torch.tensor(X_train_s.values, dtype=torch.float32)
y_train_tensor_s = torch.tensor(Y_train_s.values, dtype=torch.float32)


# Creating dataset for the KAN from the data

# x = torch.rand(100,2)
# y = torch.rand(100,1)
dataset = create_dataset_from_data(X_train_tensor_s, y_train_tensor_s, device=device)
# print(dataset)






# house_dataset_kan_train, house_dataset_kan_validation, house_dataset_kan_test = torch.utils.data.random_split(house_dataset, [0.7, 0.2, 0.1])
# print(len(house_dataset_kan_train))
# print(len(house_dataset_kan_validation))
# print(len(house_dataset_kan_test))

# print(house_dataset_kan)
# print(house_dataset_kan)

B_model_hd(dataset['train_input'])
B_model_hd.plot(beta= 100000)

A_model_hd(dataset['train_input'])
A_model_hd.plot(beta= 1)

B_model_hd.fit(dataset, opt= "LBFGS", steps= 50, lamb= 0.001)

A_model_hd.fit(dataset, opt= "LBFGS", steps= 50, lamb= 0.001)

B_model_hd.plot()

A_model_hd.plot()

model = B_model_hd.prune()
model.plot()

model_a = A_model_hd.prune()
model_a.plot()

model_a.fit(dataset, opt="LBFGS", steps=50)

model_a = model_a.refine(10)

model_a.fit(dataset, opt="LBFGS", steps=50);

mode = "auto" # "manual"

if mode == "manual":
    # manual mode
    model.fix_symbolic(0,0,0,'sin');
    model.fix_symbolic(0,1,0,'x^2');
    model.fix_symbolic(1,0,0,'exp');
elif mode == "auto":
    # automatic mode
    lib = ['x','x^2','x^3','x^4','exp','log','sqrt','tanh','sin','abs']
    model.fit(dataset, opt="LBFGS", steps=50) # Added this line to ensure activations are available
    model.auto_symbolic(lib=lib)

model.fit(dataset, opt="LBFGS", steps=50);

from kan.utils import ex_round

ex_round(model.symbolic_formula()[0][0],4)

### KAN Implementation - Study

!pip install pykan

from kan import *
import torch

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

model_k = KAN(width=[2,3,2,1], noise_scale=0.3, device=device)
x = torch.normal(0,1,size=(100,2)).to(device)
model_k(x);
beta = 100
model_k.plot(beta=beta)
# [2,3,2,1] means 2 input nodes
# 3 neurons in the first hidden layer,
# 2 neurons in the second hidden layer,
# 1 output node

# Getting Started with KANs

!pip install pykan

from kan import *
torch.set_default_dtype(torch.float64)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

# x = torch.normal(0,1,size=(100,2)).to(device)

# create a KAN: 2D inputs, 1D output, and 5 hidden neurons. cubic spline (k=3), 5 grid intervals (grid=5).
model = KAN(width=[2,5,1], grid=3, k=3, seed=42, device=device)

# model(x)

# model.plot()

from kan.utils import create_dataset
# create dataset f(x,y) = exp(sin(pi*x)+y^2)
f = lambda x: torch.exp(torch.sin(torch.pi*x[:,[0]]) + x[:,[1]]**2)
dataset = create_dataset(f, n_var=2, device=device)
dataset['train_input'].shape, dataset['train_label'].shape
# print(dataset)
# print('This is the training input data', dataset['train_input'])
# print('This are the labels', dataset['train_label'])

model(dataset['train_input'])
model.plot()

model.fit(dataset, opt= "LBFGS", steps= 50, lamb= 0.001)

model.plot()



model.plot()

model = model.prune()
model.plot()

model.fit(dataset, opt="LBFGS", steps=50)

model = model.refine(10)

model.fit(dataset, opt="LBFGS", steps=50);

mode = "auto" # "manual"

if mode == "manual":
    # manual mode
    model.fix_symbolic(0,0,0,'sin');
    model.fix_symbolic(0,1,0,'x^2');
    model.fix_symbolic(1,0,0,'exp');
elif mode == "auto":
    # automatic mode
    lib = ['x','x^2','x^3','x^4','exp','log','sqrt','tanh','sin','abs']
    model.auto_symbolic(lib=lib)

model.fit(dataset, opt="LBFGS", steps=50);

from kan.utils import ex_round

ex_round(model.symbolic_formula()[0][0],4)